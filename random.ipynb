{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae7abf3-f87f-4e7a-be59-1495e2b3436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 RANDOM BASELINE COMPRESSION FOR LLM SCALING LAWS\n",
      "============================================================\n",
      "Generating random probability baseline for compression comparison\n",
      "📊 Datasets: enwik8, imagenet, librispeech\n",
      "📊 Chunks per dataset: 2048\n",
      "📊 Chunk size: 2048 bytes\n",
      "\n",
      "🎲 RANDOM BASELINE COMPRESSION EXPERIMENT\n",
      "==================================================\n",
      "🔧 Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1146cfe4281149c888066b0a56f08fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160eccb619e34b1fa739df3752aacf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc963a4ff8a0459a842a2e6a2e09a518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📥 Loading datasets...\n",
      "📥 Downloading enwik8 Wikipedia XML...\n",
      "✅ Loaded 2048 enwik8 chunks\n",
      "📥 Downloading ImageNet images...\n",
      "🔄 Generating structured image data...\n",
      "📥 Downloading LibriSpeech audio...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915cfcb7d03c4b0b81cdcd42761a8e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "librispeech_asr.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccd3731b31840a8ab19a194d3d0684d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for librispeech_asr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/librispeech_asr.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to load LibriSpeech: To support encoding audio data, please install 'soundfile'.\n",
      "✅ Loaded 2 datasets\n",
      "\n",
      "🚀 Running 4096 compression evaluations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb059c1cc0642419efb42a0717d7b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Random Compression:   0%|          | 0/4096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Processing enwik8 (2048 chunks)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ imagenet: 1.5313 ± 0.0146 (2048 chunks)\n",
      "\n",
      "🎉 Random baseline experiment completed!\n",
      "📁 Results saved to: random_baseline_results.csv\n",
      "📊 Total evaluations: 4096\n",
      "\n",
      "📊 RANDOM BASELINE SUMMARY:\n",
      "========================================\n",
      "\n",
      "ENWIK8:\n",
      "   Mean: 0.8183 ± 0.1906\n",
      "   Range: 0.5366 - 1.7031\n",
      "   Chunks: 2048\n",
      "\n",
      "IMAGENET:\n",
      "   Mean: 1.5313 ± 0.0146\n",
      "   Range: 1.4874 - 1.5788\n",
      "   Chunks: 2048\n",
      "\n",
      "✅ RANDOM BASELINE EXPERIMENT COMPLETED!\n",
      "📈 Use these results to compare against your LLM compression data\n",
      "📁 Results saved in: random_baseline_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Random Baseline Compression Script\n",
    "# Standalone script to evaluate random baseline compression on all three modalities\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Dataset libraries\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHUNK_SIZE = 2048\n",
    "NUM_CHUNKS = 2048\n",
    "CACHE_DIR = Path(\"./compression_cache\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class RandomArithmeticCoder:\n",
    "    \"\"\"\n",
    "    Implements arithmetic coding with random probability assignments as baseline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 50304, seed: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize with random probability distribution.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (Pythia default is 50304)\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Generate random probabilities that sum to 1\n",
    "        # Use Dirichlet distribution for realistic probability distribution\n",
    "        alpha = np.ones(vocab_size) * 0.1  # Small alpha for more varied distribution\n",
    "        self.random_probs = np.random.dirichlet(alpha)\n",
    "        \n",
    "        # Ensure minimum probability to avoid log(0)\n",
    "        self.random_probs = np.maximum(self.random_probs, 1e-10)\n",
    "        self.random_probs = self.random_probs / self.random_probs.sum()  # Renormalize\n",
    "        \n",
    "    def encode_sequence_random(self, tokens: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Encode token sequence using random probabilities.\n",
    "        \n",
    "        Args:\n",
    "            tokens: Input token sequence\n",
    "            \n",
    "        Returns:\n",
    "            Total compressed bits using random probabilities\n",
    "        \"\"\"\n",
    "        if len(tokens) <= 1:\n",
    "            return len(tokens) * 16.0  # Fallback for short sequences\n",
    "        \n",
    "        total_bits = 0.0\n",
    "        \n",
    "        for token_id in tokens[1:]:  # Skip first token (no prediction needed)\n",
    "            token_id = token_id.item() if isinstance(token_id, torch.Tensor) else token_id\n",
    "            \n",
    "            # Use random probability for this token\n",
    "            if token_id < self.vocab_size:\n",
    "                prob = self.random_probs[token_id]\n",
    "            else:\n",
    "                # Out of vocabulary - assign average probability\n",
    "                prob = 1.0 / self.vocab_size\n",
    "            \n",
    "            # Arithmetic coding: -log2(probability)\n",
    "            bits = -math.log2(prob)\n",
    "            total_bits += bits\n",
    "        \n",
    "        return total_bits\n",
    "\n",
    "class DatasetManager:\n",
    "    \"\"\"Manages dataset downloading and processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = CHUNK_SIZE, num_chunks: int = NUM_CHUNKS):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_chunks = num_chunks\n",
    "        self.cache_dir = CACHE_DIR\n",
    "        \n",
    "    def fetch_enwik8_chunks(self) -> List[bytes]:\n",
    "        \"\"\"Download and process enwik8 Wikipedia data.\"\"\"\n",
    "        print(\"📥 Downloading enwik8 Wikipedia XML...\")\n",
    "        \n",
    "        try:\n",
    "            enwik8_path = self.cache_dir / \"enwik8\"\n",
    "            if not enwik8_path.exists():\n",
    "                # Download enwik8\n",
    "                url = \"http://mattmahoney.net/dc/enwik8.zip\"\n",
    "                zip_path = self.cache_dir / \"enwik8.zip\"\n",
    "                \n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                with open(zip_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(self.cache_dir)\n",
    "                zip_path.unlink()\n",
    "            \n",
    "            # Read and chunk\n",
    "            with open(enwik8_path, 'rb') as f:\n",
    "                data = f.read()\n",
    "            \n",
    "            chunks = []\n",
    "            for i in range(0, min(len(data), self.num_chunks * self.chunk_size), self.chunk_size):\n",
    "                chunk = data[i:i + self.chunk_size]\n",
    "                if len(chunk) == self.chunk_size:\n",
    "                    chunks.append(chunk)\n",
    "            \n",
    "            print(f\"✅ Loaded {len(chunks)} enwik8 chunks\")\n",
    "            return chunks[:self.num_chunks]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load enwik8: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def fetch_librispeech_chunks(self) -> List[bytes]:\n",
    "        \"\"\"Download and process LibriSpeech audio data.\"\"\"\n",
    "        print(\"📥 Downloading LibriSpeech audio...\")\n",
    "        try:\n",
    "            ds = load_dataset(\n",
    "                \"librispeech_asr\", \"clean\", split=\"train.100\", streaming=True,\n",
    "                cache_dir=str(CACHE_DIR)\n",
    "            )\n",
    "            chunks: List[bytes] = []\n",
    "            buffer = bytearray()\n",
    "            for item in ds:\n",
    "                audio = item[\"audio\"]\n",
    "                data, sr = audio[\"array\"], audio[\"sampling_rate\"]\n",
    "                if sr != 16000:\n",
    "                    import librosa\n",
    "                    data = librosa.resample(data, orig_sr=sr, target_sr=16000)\n",
    "                pcm = np.clip(data * 32767, -32768, 32767).astype(np.int16).tobytes()\n",
    "                buffer.extend(pcm)\n",
    "                while len(buffer) >= self.chunk_size and len(chunks) < self.num_chunks:\n",
    "                    chunks.append(bytes(buffer[:self.chunk_size]))\n",
    "                    buffer = buffer[self.chunk_size:]\n",
    "                if len(chunks) >= self.num_chunks:\n",
    "                    break\n",
    "            print(f\"✅ Loaded {len(chunks)} LibriSpeech chunks\")\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load LibriSpeech: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def fetch_imagenet_chunks(self) -> List[bytes]:\n",
    "        \"\"\"Download and process ImageNet image patches.\"\"\"\n",
    "        print(\"📥 Downloading ImageNet images...\")\n",
    "        \n",
    "        try:\n",
    "            # Try multiple ImageNet sources\n",
    "            sources = [\"imagenet-1k\", \"ILSVRC/imagenet-1k\"]\n",
    "            \n",
    "            for source in sources:\n",
    "                try:\n",
    "                    dataset = load_dataset(source, split=\"validation\", streaming=True, trust_remote_code=True)\n",
    "                    \n",
    "                    chunks = []\n",
    "                    processed = 0\n",
    "                    \n",
    "                    for item in dataset:\n",
    "                        if len(chunks) >= self.num_chunks:\n",
    "                            break\n",
    "                        \n",
    "                        try:\n",
    "                            image = item['image']\n",
    "                            \n",
    "                            # Convert to grayscale\n",
    "                            if image.mode != 'L':\n",
    "                                image = image.convert('L')\n",
    "                            \n",
    "                            # Check size for 32x64 patch\n",
    "                            if image.size[0] < 64 or image.size[1] < 32:\n",
    "                                continue\n",
    "                            \n",
    "                            # Extract 32x64 patch\n",
    "                            start_x = np.random.randint(0, max(1, image.size[0] - 64))\n",
    "                            start_y = np.random.randint(0, max(1, image.size[1] - 32))\n",
    "                            patch = image.crop((start_x, start_y, start_x + 64, start_y + 32))\n",
    "                            \n",
    "                            # Convert to bytes\n",
    "                            patch_array = np.array(patch, dtype=np.uint8)\n",
    "                            patch_bytes = patch_array.flatten().tobytes()\n",
    "                            \n",
    "                            if len(patch_bytes) >= self.chunk_size:\n",
    "                                chunks.append(patch_bytes[:self.chunk_size])\n",
    "                            else:\n",
    "                                # Pad if needed\n",
    "                                padding = self.chunk_size - len(patch_bytes)\n",
    "                                chunks.append(patch_bytes + b'\\x00' * padding)\n",
    "                            \n",
    "                            processed += 1\n",
    "                            \n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        \n",
    "                        if processed > self.num_chunks * 2:\n",
    "                            break\n",
    "                    \n",
    "                    if len(chunks) >= self.num_chunks // 2:\n",
    "                        print(f\"✅ Loaded {len(chunks)} image chunks from {source}\")\n",
    "                        return chunks\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Fallback: Generate structured image data\n",
    "            print(\"🔄 Generating structured image data...\")\n",
    "            return self._generate_image_data()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load images: {e}\")\n",
    "            return self._generate_image_data()\n",
    "    \n",
    "    def _generate_image_data(self) -> List[bytes]:\n",
    "        \"\"\"Generate structured image-like data.\"\"\"\n",
    "        chunks = []\n",
    "        np.random.seed(42)  # Reproducible\n",
    "        \n",
    "        for i in range(self.num_chunks):\n",
    "            # Create 32x64 image with realistic structure\n",
    "            patch = np.zeros((32, 64), dtype=np.uint8)\n",
    "            \n",
    "            for y in range(32):\n",
    "                for x in range(64):\n",
    "                    # Multiple frequency components like natural images\n",
    "                    base_val = 128\n",
    "                    low_freq = 40 * math.sin(x/20) * math.cos(y/15)\n",
    "                    med_freq = 20 * math.sin(x/8) * math.sin(y/6)\n",
    "                    high_freq = 10 * math.sin(x/3) * math.cos(y/4)\n",
    "                    noise = np.random.normal(0, 15)\n",
    "                    \n",
    "                    pixel_val = base_val + low_freq + med_freq + high_freq + noise\n",
    "                    patch[y, x] = np.clip(pixel_val, 0, 255)\n",
    "            \n",
    "            # Add some structure\n",
    "            if i % 5 == 0:  # Vertical edges\n",
    "                patch[:, 30:34] = np.clip(patch[:, 30:34] + 50, 0, 255)\n",
    "            \n",
    "            patch_bytes = patch.flatten().tobytes()[:self.chunk_size]\n",
    "            if len(patch_bytes) < self.chunk_size:\n",
    "                patch_bytes += b'\\x00' * (self.chunk_size - len(patch_bytes))\n",
    "            \n",
    "            chunks.append(patch_bytes)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "def bytes_to_ascii(data_bytes: bytes) -> str:\n",
    "    \"\"\"Convert bytes to ASCII string for tokenization.\"\"\"\n",
    "    return ''.join(chr(b % 128) for b in data_bytes)\n",
    "\n",
    "def compute_random_compression_ratio(random_coder: RandomArithmeticCoder, \n",
    "                                   tokenizer, \n",
    "                                   raw_bytes: bytes) -> float:\n",
    "    \"\"\"\n",
    "    Compute compression ratio using random probabilities.\n",
    "    \n",
    "    Returns:\n",
    "        compression_ratio: compressed_bits / original_bits\n",
    "    \"\"\"\n",
    "    # Convert bytes to ASCII text\n",
    "    ascii_text = bytes_to_ascii(raw_bytes)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(ascii_text, add_special_tokens=False, max_length=1024, truncation=True)\n",
    "    tokens_tensor = torch.tensor(tokens)\n",
    "    \n",
    "    if len(tokens_tensor) < 2:\n",
    "        return 1.0  # No compression possible\n",
    "    \n",
    "    # Compute compressed size using random probabilities\n",
    "    compressed_bits = random_coder.encode_sequence_random(tokens_tensor)\n",
    "    \n",
    "    # Original size in bits\n",
    "    original_bits = len(raw_bytes) * 8\n",
    "    \n",
    "    # Compression ratio\n",
    "    compression_ratio = compressed_bits / original_bits\n",
    "    \n",
    "    return compression_ratio\n",
    "\n",
    "def run_random_baseline_experiment():\n",
    "    \"\"\"Run random baseline compression experiment.\"\"\"\n",
    "    print(\"🎲 RANDOM BASELINE COMPRESSION EXPERIMENT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"🔧 Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\", trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Initialize random coder\n",
    "    random_coder = RandomArithmeticCoder(vocab_size=tokenizer.vocab_size, seed=42)\n",
    "    \n",
    "    # Load datasets\n",
    "    dataset_manager = DatasetManager()\n",
    "    print(\"\\n📥 Loading datasets...\")\n",
    "    datasets = {\n",
    "        'enwik8': dataset_manager.fetch_enwik8_chunks(),\n",
    "        'imagenet': dataset_manager.fetch_imagenet_chunks(),\n",
    "        'librispeech': dataset_manager.fetch_librispeech_chunks(),\n",
    "    }\n",
    "    \n",
    "    # Filter successful datasets\n",
    "    datasets = {k: v for k, v in datasets.items() if v}\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"❌ No datasets available!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"✅ Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    # Run experiments\n",
    "    results = []\n",
    "    total_experiments = sum(len(chunks) for chunks in datasets.values())\n",
    "    \n",
    "    print(f\"\\n🚀 Running {total_experiments} compression evaluations...\")\n",
    "    \n",
    "    with tqdm(total=total_experiments, desc=\"Random Compression\") as pbar:\n",
    "        for dataset_name, chunks in datasets.items():\n",
    "            print(f\"\\n📊 Processing {dataset_name} ({len(chunks)} chunks)...\")\n",
    "            \n",
    "            compression_ratios = []\n",
    "            \n",
    "            for i, chunk_bytes in enumerate(chunks):\n",
    "                try:\n",
    "                    ratio = compute_random_compression_ratio(random_coder, tokenizer, chunk_bytes)\n",
    "                    if 0.01 < ratio < 50.0:  # Sanity check\n",
    "                        compression_ratios.append(ratio)\n",
    "                        \n",
    "                        # Store individual result\n",
    "                        results.append({\n",
    "                            'dataset': dataset_name,\n",
    "                            'chunk_id': i,\n",
    "                            'compression_ratio': ratio,\n",
    "                            'model': 'random_baseline',\n",
    "                            'timestamp': time.time()\n",
    "                        })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Update progress info\n",
    "                if len(compression_ratios) > 0:\n",
    "                    current_mean = np.mean(compression_ratios)\n",
    "                    pbar.set_postfix({\n",
    "                        'dataset': dataset_name,\n",
    "                        'current_ratio': f'{current_mean:.3f}'\n",
    "                    })\n",
    "            \n",
    "            # Print dataset summary\n",
    "            if compression_ratios:\n",
    "                mean_ratio = np.mean(compression_ratios)\n",
    "                std_ratio = np.std(compression_ratios)\n",
    "                print(f\"   ✅ {dataset_name}: {mean_ratio:.4f} ± {std_ratio:.4f} ({len(compression_ratios)} chunks)\")\n",
    "            else:\n",
    "                print(f\"   ❌ {dataset_name}: No valid results\")\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"random_baseline_results.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\n🎉 Random baseline experiment completed!\")\n",
    "    print(f\"📁 Results saved to: random_baseline_results.csv\")\n",
    "    print(f\"📊 Total evaluations: {len(results)}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📊 RANDOM BASELINE SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for dataset in df['dataset'].unique():\n",
    "        subset = df[df['dataset'] == dataset]\n",
    "        mean_cr = subset['compression_ratio'].mean()\n",
    "        std_cr = subset['compression_ratio'].std()\n",
    "        min_cr = subset['compression_ratio'].min()\n",
    "        max_cr = subset['compression_ratio'].max()\n",
    "        \n",
    "        print(f\"\\n{dataset.upper()}:\")\n",
    "        print(f\"   Mean: {mean_cr:.4f} ± {std_cr:.4f}\")\n",
    "        print(f\"   Range: {min_cr:.4f} - {max_cr:.4f}\")\n",
    "        print(f\"   Chunks: {len(subset)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎲 RANDOM BASELINE COMPRESSION FOR LLM SCALING LAWS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Generating random probability baseline for compression comparison\")\n",
    "    print(f\"📊 Datasets: enwik8, imagenet, librispeech\")\n",
    "    print(f\"📊 Chunks per dataset: {NUM_CHUNKS}\")\n",
    "    print(f\"📊 Chunk size: {CHUNK_SIZE} bytes\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        results_df = run_random_baseline_experiment()\n",
    "        \n",
    "        if results_df is not None:\n",
    "            print(\"\\n✅ RANDOM BASELINE EXPERIMENT COMPLETED!\")\n",
    "            print(\"📈 Use these results to compare against your LLM compression data\")\n",
    "            print(\"📁 Results saved in: random_baseline_results.csv\")\n",
    "        else:\n",
    "            print(\"❌ Experiment failed\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏸️  Experiment interrupted by user\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Experiment failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6443a-c8b8-4edd-baaf-156d47251444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
